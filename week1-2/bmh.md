# Linear Regression with One Variable

## Model Representation
- 우리가 할 일은 데이터셋으로부터 어떻게 결과를 예측할지 학습시키는 것ㅎ

#### 회귀문제 예시 - 집 사이즈에 따라 판매가 예측하기
- 데이터를 통해 `적합한 답`을 준다
#### 분류문제 예시 - 종양이 악성인지 아닌지 결정하기
- 결정을 내리는 것과 같은 이산값(0,1)을 예측하는 문제

---

### 데이터 표기법
- m = 학습 예제의 개수
- x = input
- y = output
- x<sup>(i)</sup> = x의 i번째 데이터(순서를 나타냄)
- y<sup>(i)</sup> = y의 i번째 데이터(순서를 나타냄)
- h = hopothesis = x의 입력으로 y의 결과가 도출되는 가설

단일변량 -> 하나의 값 -> 결국 1차함수 -> y = ax + b의 형태

---

## Cost Function(비용함수)
- 비용함수의 목적은 오차를 작게 만들어주는 theta_0와 theta_1을 찾는 것
- theta_0와 theta_1을 잘 결정한다는 것
⇒ (X, Y) 의 좌표로 나타낸 점들에 가장 가까이 있는 직선을 찾는다는 것으로 바꾸어 말할 수 있다.
곧, 모든 점들과의 거리의 합이 최소가 되는 직선을 찾는 것
- 오차함수의 제곱이라고 이야기 함
- 일차함수의 형태가 모든 데이터셋을 지나면 J(θ<sub>0</sub>, θ<sub>1</sub>) = 0임 -> 오차가 없다
- 즉 목표가 오차가 없는 형태(기울기0, 미분값0)
- θ<sub>0</sub> = 0 이라고 가정하고 원점을 지나는 일차함수의 형태로 가설을 세우면 비용함수
J를 더 쉽게 그릴수 있다(2차함수의 형태)
- θ<sub>0</sub>가 0이 아니면 등고선 그래프를 활용해야함
 - 같은 등고선은 같은 높이를 가지고 있다고
 - 등고선도 그래프에서는 결국 타원의 중심이 최소값
   
## Gradient Descent(기울기 하강 알고리즘)
- 비용함수 J의 최소값을 구하는 알고리즘, 즉 오차를 작게하는 값을 구하는 것
- 중요한 것은 한번에 계산해야함
  - θ<sub>0</sub>을 구한 후 θ<sub>0</sub>에 대입하여 θ<sub>1</sub>을 구하면 안됨(다음 값에 영향이 있음)
  - θ<sub>0</sub>, θ<sub>1</sub>을 구하여 한 번에 적용해야함
  
### learning rate(alpha)
- moving step
- 너무 작을 경우에는 많은 이동이 필요함, 즉 많은 시간이 소요
- 너무 클 경우에는 계속해서 최소값에서 멀어지게 됨
- learning rate를 강제로 감소시킬 필요가 없음, θ<sub>1</sub>의 값이 작아지면서 기울기(미분계수)가
작아지기때문에 값이 자연스럽게 작아지기 때문에
